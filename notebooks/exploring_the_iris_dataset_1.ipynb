{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First look at the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get underway the analysis on this dataset and start this project, it is assumed that I have no prior knowledge of this dataset (_While I have encountered this dataset in other coursework modules this semester_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file containing the datset can be downloaded from the UC Irvine Machine Learning Repository: https://archive.ics.uci.edu/dataset/53/iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pandas module for the DataFrame\n",
    "import pandas as pd \n",
    "\n",
    "#load local cvs file located in .\\resources\\iris.data\n",
    "iris = pd.read_csv(\"resources/iris.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can load this dataset directly within Python by importing the **datasets** submodule from the **scikit-learn** module  \n",
    "https://scikit-learn.org/1.4/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the datasets submodule (within the sklearn module)\n",
    "from sklearn import datasets  \n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we can begin to look at dataset structure ...  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections: Lists, Arrays, Datasets, Dataframes, and Bunches (???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the wide array of \"container\" data types available in Python (and extended modules), it is important to identify what what types we are dealing with when initially opening datasets.  \n",
    "\n",
    "If you choose the load dataset from a comma-separated values (csv) files with **pandas.read_csv()**, it wil return a two-dimensional data structure from the **pandas** module known as a \"dataframe\"   \n",
    "https://pandas.pydata.org/docs/reference/frame.html  \n",
    "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "\n",
    "Alteratively, if you choose the  **sklearn.datasets.load_iris()** approach, it will load the iris dataset and returns by default a Dictionary-like \"Bunch\" object  \n",
    "https://scikit-learn.org/0.24/modules/generated/sklearn.utils.Bunch.html\n",
    "\n",
    "> *By Default, sklearn.datasets.load_iris() loads a Bunch object, but you can load a dataframe if needed with Parameter as_frame=True\n",
    "\n",
    "This Bunch object contains multiple attributes:\n",
    "\n",
    "* The data is a **numpy.ndarray** (2D array for features).\n",
    "* The target is a **numpy.ndarray** (1D array for class labels).\n",
    "* The feature_names and target_names are lists of **strings**.\n",
    "* The DESCR is a **str** (detailed description of the dataset).\n",
    "\n",
    "These data attributes are further explored in more detail in [this notebook](Bunch_attributes.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are examples of conversions between the various datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris type is <class 'sklearn.utils._bunch.Bunch'>\n",
      "iris.data type is <class 'numpy.ndarray'>\n",
      "iris_arr type is <class 'numpy.ndarray'>\n",
      "iris_data_frame type is <class 'pandas.core.frame.DataFrame'>\n",
      "iris_data_list type is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#load the Bunch object\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Convert Bunch to DataFrame with the data attribute (numpy.ndarray)\n",
    "iris_data_frame = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "#Convert Dataframe to numpy ndarray\n",
    "iris_arr = iris_data_frame.to_numpy()\n",
    "\n",
    "# Convert the data (numpy ndarray) to a list\n",
    "iris_data_list = iris.data.tolist()\n",
    "\n",
    "print (f\"iris type is {type(iris)}\")\n",
    "print(f\"iris.data type is {type(iris.data)}\")\n",
    "print(f\"iris_arr type is {type(iris_arr)}\")\n",
    "print(f\"iris_data_frame type is {type(iris_data_frame)}\")\n",
    "print (f\"iris_data_list type is {type(iris_data_list)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Shape of the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both **pandas.dataframe** and **numpy.ndarray** expose a \"shape\" property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If reviewing the shape of the data returned from a **pandas read_cvs()** operation, then the shape is a 2D array of size 150x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "iris_csv = pd.read_csv(\"resources/iris.csv\")\n",
    "print(iris_csv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note: I experienced an issue with another CSV file downloaded from UCI Machine Learning Repository.   \n",
    "The Shape was only 149,5. It was ignoring the first data row. This was because the file was a \"data\" file not a \"csv\" file that did not have the feature names in first header row. As per csv file downloaded from github*  \n",
    "\n",
    "https://gist.github.com/netj/8836201#file-iris-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bunch object returned from **sklearn.datasets.load_iris()** contains several attributes besides the core \"data\".  \n",
    "Using the numpy.ndarray shape attribute, we can extract size of Iris dataset.  \n",
    "We can also look at the \"target\" data (also a numpy.ndarray)\n",
    "\n",
    "The \"data\" attribute is a 2D array of 150 x 4 floating point data points (150 data items with 4 attributes)  \n",
    "The \"target\" attribute is  1D array of 150 x 1 data points with enumerations codes 1,2, or 3 (150 data items with a single attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#view the shape of data and target attributes\n",
    "print(iris.data.shape)\n",
    "print(iris.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get further context on what the 4 data attributes are ...   \n",
    "The Bunch object has \"feature_names\" with 4 data records that gives names the dataset's features.  \n",
    "\n",
    "While **feature_names** lists the names of the dataset's features, the **target_names** lists the names of the classes (species) enumerated in the dataset's target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#view the raw data in feature_names and target_names attributes\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the DESCR Attribute that is a long string that contains a full detailed description of the iris dataset.   \n",
    "The size of the DESCR string is 1065 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Null Values\n",
    "\n",
    "We should test this Dataset for any null values ...  \n",
    "While dataframes support an isnull() method, numpy arrays do not support the isnull() method.\n",
    "When testing for nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Values :\n",
      "sepal.length    0\n",
      "sepal.width     0\n",
      "petal.length    0\n",
      "petal.width     0\n",
      "variety         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#load the iris dataframe\n",
    "iris_df = pd.read_csv(\"resources/iris.csv\")\n",
    "\n",
    "# Check for missing values and sum over the 4 features\n",
    "null_values = iris_df.isnull().sum()\n",
    "\n",
    "print(f\"Number of Null Values :\\n{null_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# First Look Conclusion\n",
    "The iris dataset can loaded using the **pandas.read_csv()** or **sklearn.datasets.load_iris()** method.   \n",
    "\n",
    "The iris dataset extracted from the  **sklearn.datasets.load_iris()** is not a simple array, list, or dictionary.  \n",
    "Its contains the following attributes :   \n",
    "   - `data` → **`numpy.ndarray`** (2D array with features)\n",
    "   - `target` → **`numpy.ndarray`** (1D array with target labels)\n",
    "   - `feature_names` → **`list`** (list of feature names)\n",
    "   - `target_names` → **`list`** (list of target class names)\n",
    "   - `DESCR` → **`str`** (string description)\n",
    "\n",
    "The core data that was collected by Ronald Fisher in 1936 is contained with the 'data' ndarray.  \n",
    "It is 2D array of 150 x 4 data points with no null values. *i.e 4 specific features were sampled on 150 plants.*\n",
    "1. sepal length (cm)\n",
    "2. sepal width (cm)\n",
    "3. petal length (cm)\n",
    "4. petal width (cm)   \n",
    "\n",
    "These 4 features are visualized on below image of a typical Iris flower :   \n",
    "![alt text](images\\sepals_and_petals_600w.webp)\n",
    "\n",
    "The 4 names of each these features are listed with the feature_names attribute\n",
    "\n",
    "\n",
    "The result of the statistical classification done on this core data is stored in the target attribute.  \n",
    "Based on the resultant measurements taken on these four features, statistical or machine learning methods could distinguish 3 unique species.\n",
    "Each of the 150 measured plants were classified into 3 distinct classes or species :\n",
    "1. setosa\n",
    "2. versicolor\n",
    "3. virginica\n",
    "\n",
    "The target attribute array stores a number (1-3) where each number represents the class or species name\n",
    "\n",
    "**Any processing on core 'data' or 'target' arrays need both feature_names and target_names to give data contextual meaning.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
